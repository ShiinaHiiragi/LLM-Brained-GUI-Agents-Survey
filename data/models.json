[
    {
        "Name": "Attention-driven GUI Grounding: Leveraging Pretrained Multimodal Large Language Models without Fine-Tuning",
        "Platform": "General",
        "Date": "December 2024",
        "Paper_Url": "https://arxiv.org/abs/2412.10840",
        "Highlight": "TAG: a Tuning-free Attention-driven GUI Grounding method for GUI task automation",
        "Code_Url": "https://github.com/HeimingX/TAG.git"
    },
    {
        "Name": "Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents",
        "Platform": "Web",
        "Date": "August 2024",
        "Paper_Url": "http://arxiv.org/abs/2408.07199v1",
        "Highlight": "Combines Monte Carlo Tree Search (MCTS) with self-critique mechanisms, leveraging reinforcement learning to achieve exceptional performance",
        "Code_Url": "https://github.com/sentient-engineering/agent-q"
    },
    {
        "Name": "Search Beyond Queries: Training Smaller Language Models for Web Interactions via Reinforcement Learning",
        "Platform": "Web",
        "Date": "April 2024",
        "Paper_Url": "http://arxiv.org/abs/2404.10887v1",
        "Highlight": "Efficient use of smaller LLMs, and integration of RL and human demonstrations for robust performance",
        "Code_Url": "/"
    },
    {
        "Name": "Navigating WebAI: Training Agents to Complete Web Tasks with Large Language Models and Reinforcement Learning",
        "Platform": "Web",
        "Date": "May 2024",
        "Paper_Url": "http://arxiv.org/abs/2405.00516v1",
        "Highlight": "Combines supervised learning and reinforcement learning to address limitations of previous models in memorization and generalization",
        "Code_Url": "/"
    },
    {
        "Name": "OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization",
        "Platform": "Web",
        "Date": "October 2024",
        "Paper_Url": "http://arxiv.org/abs/2410.19609v1",
        "Highlight": "Combining imitation learning with a feedback loop for continuous improvement",
        "Code_Url": "https://github.com/MinorJerry/OpenWebVoyager"
    },
    {
        "Name": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning",
        "Platform": "Web",
        "Date": "November 2024",
        "Paper_Url": "http://arxiv.org/abs/2411.02337v1",
        "Highlight": "Introduces a self-evolving online curriculum reinforcement learning framework, which dynamically generates tasks based on past failures and adapts to the agent's skill level",
        "Code_Url": "https://github.com/THUDM/WebRL"
    },
    {
        "Name": "Multimodal Web Navigation with Instruction-Finetuned Foundation Models",
        "Platform": "Web",
        "Date": "May 2023",
        "Paper_Url": "http://arxiv.org/abs/2305.11854v4",
        "Highlight": "Integrates temporal and local multimodal perception, combining HTML and visual tokens, and uses an instruction-finetuned language model for enhanced reasoning and task generalization",
        "Code_Url": "https://console.cloud.google.com/storage/browser/gresearch/webllm"
    },
    {
        "Name": "MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding",
        "Platform": "Android",
        "Date": "September 2024",
        "Paper_Url": "http://arxiv.org/abs/2409.14818v2",
        "Highlight": "Mobile-specific pretraining tasks that enhance intra- and inter-UI understanding, with a uniquely large and graph-structured Chinese UI dataset (Mobile3M)",
        "Code_Url": "https://github.com/XiaoMi/mobilevlm"
    },
    {
        "Name": "Octo-planner: On-device Language Model for Planner-Action Agents",
        "Platform": "Mobile devices",
        "Date": "June 2024",
        "Paper_Url": "http://arxiv.org/abs/2406.18082v1",
        "Highlight": "Optimized for resource-constrained devices to ensure low latency, privacy, and offline functionality",
        "Code_Url": "https://huggingface.co/NexaAIDev/octopus-planning"
    },
    {
        "Name": "DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning",
        "Platform": "Android",
        "Date": "June 2024",
        "Paper_Url": "http://arxiv.org/abs/2406.11896v1",
        "Highlight": "Offline-to-online reinforcement learning, bridging gaps in static and dynamic environments",
        "Code_Url": "https://github.com/DigiRL-agent/digirl"
    },
    {
        "Name": "Visual Grounding for User Interfaces",
        "Platform": "Android",
        "Date": "June 2024",
        "Paper_Url": "https://aclanthology.org/2024.naacl-industry.9",
        "Highlight": "Unifies detection and grounding tasks through layout-guided contrastive learning",
        "Code_Url": "/"
    },
    {
        "Name": "Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs",
        "Platform": "Android and iPhone platforms",
        "Date": "April 2024",
        "Paper_Url": "http://arxiv.org/abs/2404.05719v1",
        "Highlight": "Multi-platform support with high-resolution adaptive image encoding",
        "Code_Url": "https://github.com/apple/ml-ferret/tree/main/ferretui"
    },
    {
        "Name": "Octopus: On-device language model for function calling of software APIs",
        "Platform": "Mobile devices",
        "Date": "April 2024",
        "Paper_Url": "http://arxiv.org/abs/2404.01549v1",
        "Highlight": "Use of conditional masking to enforce correct output formatting",
        "Code_Url": "/"
    },
    {
        "Name": "Octopus v2: On-device language model for super agent",
        "Platform": "Edge devices",
        "Date": "April 2024",
        "Paper_Url": "http://arxiv.org/abs/2404.01744v5",
        "Highlight": "Functional tokenization strategy significantly reduces context length required for accurate prediction",
        "Code_Url": "/"
    },
    {
        "Name": "Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent",
        "Platform": "Edge devices",
        "Date": "April 2024",
        "Paper_Url": "http://arxiv.org/abs/2404.11459v2",
        "Highlight": "Introduction of functional tokens for multimodal applications enhances the model's flexibility",
        "Code_Url": "/"
    },
    {
        "Name": "Octopus v4: Graph of language models",
        "Platform": "Serverless cloud-based platforms and edge devices",
        "Date": "April 2024",
        "Paper_Url": "http://arxiv.org/abs/2404.19296v1",
        "Highlight": "Graph-based framework integrating multiple specialized models for optimized performance",
        "Code_Url": "https://github.com/NexaAI/octopus-v4"
    },
    {
        "Name": "VGA: Vision GUI Assistant -- Minimizing Hallucinations through Image-Centric Fine-Tuning",
        "Platform": "Android",
        "Date": "June 2024",
        "Paper_Url": "https://arxiv.org/abs/2406.14056",
        "Highlight": "Minimizes hallucinations in GUI comprehension using image-centric fine-tuning approach, balancing text and visual content",
        "Code_Url": "https://github.com/Linziyang1999/VGA-visual-GUI-assistant"
    },
    {
        "Name": "MobileFlow: A Multimodal LLM For Mobile GUI Agent",
        "Platform": "Mobile phones",
        "Date": "July 2024",
        "Paper_Url": "http://arxiv.org/abs/2407.04346v2",
        "Highlight": "Hybrid visual encoder with variable-resolution input and Mixture of Experts (MoE) for enhanced performance and efficiency",
        "Code_Url": "/"
    },
    {
        "Name": "UINav: A Practical Approach to Train On-Device Automation Agents",
        "Platform": "Android",
        "Date": "December 2023",
        "Paper_Url": "http://arxiv.org/abs/2312.10170v4",
        "Highlight": "Macro action framework and error-driven demonstration collection process enable robust performance with small, efficient models",
        "Code_Url": "/"
    },
    {
        "Name": "ScreenAgent: A Vision Language Model-driven Computer Control Agent",
        "Platform": "Linux and Windows Computer",
        "Date": "February 2024",
        "Paper_Url": "http://arxiv.org/abs/2402.07945v1",
        "Highlight": "Comprehensive pipeline of planning, acting, and reflecting to handle real computer screen operations autonomously",
        "Code_Url": "https://github.com/niuzaisheng/ScreenAgent"
    },
    {
        "Name": "Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API",
        "Platform": "Computer",
        "Date": "October 2023",
        "Paper_Url": "http://arxiv.org/abs/2310.04716v1",
        "Highlight": "Incorporates reinforcement learning with environmental feedback for structured and goal-oriented tasks",
        "Code_Url": "https://choiszt.github.io/Octopus/"
    },
    {
        "Name": "CogAgent: A Visual Language Model for GUI Agents",
        "Platform": "Mobile and Computer",
        "Date": "December 2023",
        "Paper_Url": "http://arxiv.org/abs/2312.08914v2",
        "Highlight": "Innovatively uses policy gradients to improve spatial decoding in the pixel-to-sequence paradigm",
        "Code_Url": "https://github.com/THUDM/CogVLM"
    },
    {
        "Name": "SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents",
        "Platform": "PC, web, and Android platforms",
        "Date": "January 2024",
        "Paper_Url": "http://arxiv.org/abs/2401.10935v2",
        "Highlight": "High-resolution cross-module to balance computational efficiency and high-resolution input processing",
        "Code_Url": "https://github.com/njucckevin/SeeClick"
    },
    {
        "Name": "ScreenAI: A Vision-Language Model for UI and Infographics Understanding",
        "Platform": "iOS, Android, macOS, Windows, and web",
        "Date": "February 2024",
        "Paper_Url": "http://arxiv.org/abs/2402.04615v3",
        "Highlight": "Ability to perform GUI tasks purely from screenshots and its novel GUI grounding pre-training approach",
        "Code_Url": "https://github.com/kyegomez/ScreenAI"
    },
    {
        "Name": "Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms",
        "Platform": "Mobile, Computer, and tablet UIs",
        "Date": "October 2024",
        "Paper_Url": "http://arxiv.org/abs/2410.18967v1",
        "Highlight": "Unified representation of UIs and infographics, combining visual and textual elements",
        "Code_Url": "/"
    },
    {
        "Name": "OmniParser for Pure Vision Based GUI Agent",
        "Platform": "iPhone, Android, iPad, Web, AppleTV",
        "Date": "August 2024",
        "Paper_Url": "http://arxiv.org/abs/2408.00203v1",
        "Highlight": "Multi-platform support with high-resolution adaptive image encoding",
        "Code_Url": "/"
    },
    {
        "Name": "OmniParser for Pure Vision Based GUI Agent",
        "Platform": "Mobile, Computer, and Web",
        "Date": "August 2024",
        "Paper_Url": "https://arxiv.org/abs/2408.00203",
        "Highlight": "Introduces a vision-only screen parsing framework, enabling general UI understanding without reliance on external information",
        "Code_Url": "https://github.com/microsoft/OmniParser"
    },
    {
        "Name": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents",
        "Platform": "Websites, Computers, and mobile phones",
        "Date": "October 2024",
        "Paper_Url": "http://arxiv.org/abs/2410.23218v1",
        "Highlight": "Introduces a multi-platform foundation action model, along with a large GUI grounding dataset",
        "Code_Url": "https://github.com/OS-Copilot/OS-Atlas"
    },
    {
        "Name": "xLAM: A Family of Large Action Models to Empower AI Agent Systems",
        "Platform": "Windows, macOS, Linux, Android, and the web",
        "Date": "September 2024",
        "Paper_Url": "http://arxiv.org/abs/2409.03215v1",
        "Highlight": "First foundation action model for generalist GUI agents, supporting cross-platform GUI tasks, with a unified action space",
        "Code_Url": "https://osatlas.github.io/"
    },
    {
        "Name": "Falcon-UI: Understanding GUI Before Following User Instructions",
        "Platform": "iOS, Android, Windows, Linux, Web",
        "Date": "December 2024",
        "Paper_Url": "https://arxiv.org/abs/2412.09362",
        "Highlight": "Decouples GUI context comprehension from instruction-following tasks, leveraging an instruction-free pretraining approach.",
        "Code_Url": ""
    },
    {
        "Name": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent",
        "Platform": "Desktop, Mobile, Web",
        "Date": "November 2024",
        "Paper_Url": "https://arxiv.org/abs/2411.17465",
        "Highlight": "Lightweight Vision-Language-Action (VLA) model supporting both GUI grounding and navigation tasks",
        "Code_Url": "https://github.com/showlab/ShowUI"
    }
]